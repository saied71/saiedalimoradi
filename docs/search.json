[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Saied Alimoradi Blog",
    "section": "",
    "text": "How to use HuggingFace datasets library\n\n\n\n\n\nThis post is a quick HOWTO for huggingface datasets library including preprocessing and batch processing the huggingface dataset with usecase of persian_news_dataset\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nEstonian ULMFIT\n\n\n\n\n\nThis notebook contains codes for finetunning AWD-LSTM language model Using ULMFIT Approach for text classification in Estonian Language\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\ntest-notebook\n\n\n\n\n\nnothing\n\n\n\n\n\n\nJul 17, 2024\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nPretraining-Persian-AWD-LSTM-Language-model\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nPersian ULMFIT\n\n\n\n\n\nThis notebook contains codes for finetunning AWD-LSTM language model Using ULMFIT Approach for sentiment analysis in Persian Language\n\n\n\n\n\n\nJul 17, 2021\n\n\n\n\n\n\n  \n\n\n\n\nPretraining Persian AWD-LSTM Language model\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nAn overview of pretraining language model and use it for classification with the ULMFIT approach in Persian language model\n\n\n\n\n\n\nJul 17, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-8-24-ulmfit-et.html",
    "href": "posts/2021-8-24-ulmfit-et.html",
    "title": "Estonian ULMFIT",
    "section": "",
    "text": "In this post, I want to finetune our language model and doing text classification using ULMFIT approach for Estonian language. Note on the Language model:\nHere I use the AWD-LSTM language model that I already trained. The procedure of the language model is almost the same as Pretraining Persian AWD-LSTM Language model except for the data. I used Oscar dataset, and after cleaning, I’ve got around 200k articles (~800 MB). for training, it almost took 15 hours on p3 instance of AWS for 10 epochs. Here are the metrics for the last epoch:\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\n\n\n\n\n9\n4.3451\n4.3609\n0.29820\n78.3298\n\n\n\nFor fine-tuning, I couldn’t get a standard dataset for benchmarking, so I crawled some news data(around 2700) articles for the sake of demonstration. Maybe in the future, I gathered such a dataset that also can be useful for other tasks. the classes of data are:\n\npoliitika(politics)\nkultuur(culture)\nmajandus(economy)\n\nAlright, let’s get started.\nFirst, we download the model, tokenizer, and the data, which we will use to finetune our language model. after this running this cell, a file named ULMFIT_ET.zip will be downloaded, which includes the model, tokenizer, and the data.\n\n%%capture\nimport gdown\nurl =  \"https://drive.google.com/uc?id=1yWUixE3SpALPtaJjeHdCDIg5bAkhGbz0\"\noutput = 'ULMFIT_ET.zip'\ngdown.download(url, output, quiet=False)\n!unzip ULMFIT_ET.zip\n!pip install -U fastai\n!pip install sentencepiece\n\n\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.text.all import *\nimport pandas as pd\nimport pickle\n\nimport fastai\nimport torch\nprint(f\"fastai version: {fastai.__version__}\")\nprint(f\"GPU which is used : {torch.cuda.get_device_name(0)}\")\n\n## parameters for dataloader and tokenizer\nlang = \"et\"\nbackwards=False\nbs=128\nvocab_sz = 30000\ndrop_mult = 0.5\nnum_workers=18\n## setting up the pathes\nbase = Path(\".\").absolute()\nprint(f\"our base directory: {base}\")\nulmfit_dir = base / \"ULMFIT_ET\"\nprint(f\"our model and data directory: {ulmfit_dir}\")\nlm_fns = [ulmfit_dir / f\"model_out/{lang}_ULMFIT\", ulmfit_dir / f\"model_out/{lang}_ULMFIT_vocab\"]\n\nfastai version: 2.4.1\nGPU which is used : Tesla P100-PCIE-16GB\nour base directory: /content\nour model and data directory: /content/ULMFIT_ET\n\n\n\ndf = pd.read_csv(ulmfit_dir / \"data_finetune.csv\")\nprint(f\"shape of the data: {df.shape}\")\ndf.sample(5)\n\nshape of the data: (2735, 2)\n\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n1016\nMajandus- ja taristuministri Taavi Aasa hinnangul on veeteetasude langetamise otsus end õigustanud ning seda tuleks teha ka järgnevatel aastatel. Esmaspäeval toimus riigikogu transiidi ja logistika toetusrühma koosolek, kus räägiti aktsiiside ja veeteede tasude mõjust majandusele, riigieelarvele ja ettevõtluse konkurentsivõimele. Arutelul osalesid lisaks Aasale riigikogu liikmed ning rahandusministeeriumi ja ettevõtjate esindajad. “Tänu veeteetasude vähendamisele on Eesti sadamate kaubamaht kumulatiivselt kasvanud 10 protsenti ning tänu kütuseaktsiiside langetusele on paljud ettevõtted taa...\nmajandus\n\n\n721\nSotsiaalministeerium saatis ministeeriumitele ja partneritele kooskõlastamiseks bioloogiliste ohutegurite määruse muutmise eelnõu. Muudatustega uuendatakse töökeskkonna bioloogiliste ohutegurite ohurühmade loetelu, kuhu lisanduvad uued bioloogilised ohutegurid, näiteks koroonaviirus SARS-CoV-2. Muudatused võimaldavad nõuetekohaselt kaitsta töökohtadel töötajate tervist ja tagada ohutust viiruse leviku ajal.„Bioloogilised ohutegurid võivad põhjustada töötajatel erinevaid haigusi – allergiaid, mürgistusi ja nakkuseid. Seega tuleb töökeskkonnas viia töötajate kokkupuude bioloogiliste ohutegur...\npoliitika\n\n\n2344\n„Teater on üks väga lihtne asi. Muud kunsti pole, kui õpid sõnad pähe, ja siis oled näoga rahva poole ja ütled kõva häälega. Ja ajanäitajale pole vaja vaadata. Publiku nägudelt on näha, kui on aeg lõpetada. Ja kui te minuga nõus olete, siis pean ütlema, et teil on hea maitse.” Nii on tänane juubilar, rahvanäitleja Endel Pärn kirjutanud tervitussõnadeks Marvi Taggo raamatule “Endel Pärn. Operetiprofessor” (Faatum 1998).Endel Pärn sündis 21. aprillil 1914 Petrogradis Vassili saarel 4. liinil Aleksander Pärna esimese pojana. Noorema venna nimi oli Ants, ema oli Helene. Endel Pärn on meenutanu...\nkultuur\n\n\n1867\n16. augustil kiitis Vabariigi Valitsus heaks Eesti maaelu arengukava 2014–2020 kolmanda muudatuse, millega muu hulgas lisati arengukavva uus riskijuhtimise toetusmeede. Samuti suurendatakse mahepõllumajanduse toetuse eelarvet, põllumajandusliku tootmise potentsiaali taastamise toetuse eelarvet ning suunatakse lisaraha arengukava rahastamisvahendisse. „Riskijuhtimise meede on uus toetusskeem, mis on ette nähtud erinevate põllumajandustootmist mõjutavate riskidega tegelemiseks. On oluline, et põllumehed tegeleksid riskijuhtimisega, mis aitaks leevendada ilmakahjusid ja tagasilööke turgudel,“...\nmajandus\n\n\n481\nStenbocki maja, 15. detsember 2020 – Peaminister Jüri Ratase sõnul kujutab koroonaviiruse levik Eesti inimeste tervisele ning ühiskonna ja majanduse toimimisele jätkuvalt ohtu ning viiruse leviku pidurdamine sõltub igast inimesest, kuid kui kevadel andis meile jõudu lähenev suvi, siis nüüd saame loota saabuvatele vaktsiinidele.Ratas ütles riigikogu ees tehtud poliitilises avalduses, et pea seitse kuud pärast kevadise eriolukorra lõppu oleme suuresti õppinud koroonaviirusega elama. “Kooseluoskust peame me veel mõnda aega arendama ning seda läheb tarvis, kuni saame vaktsiinide, ravimite ning...\npoliitika\n\n\n\n\n\n\n\nUsing pretrained SentencePiece for tokenization. Then create a data loader for feeding the language model learner.\n\ntok = SentencePieceTokenizer(lang=\"et\", max_vocab_sz=vocab_sz, sp_model=ulmfit_dir / \"spm/spm.model\")\n\n\ndblock_lm = DataBlock(\n    blocks=(TextBlock.from_df('text', is_lm=True, tok=tok,backwards=False)),\n    get_x=ColReader('text'))\n\n\ndls_lm = dblock_lm.dataloaders(df, bs=bs)\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  return array(a, dtype, copy=False, order=order)\n\n\n\ndls_lm.show_batch(max_n=4)\n\n\n\n\n\ntext\ntext_\n\n\n\n\n0\n▁xxbos ▁äsja ▁avaldatud ▁xxup ▁oecd ▁haridus statistika ▁kogumik ▁xxmaj ▁edu c ation ▁ at ▁a ▁xxmaj ▁ gla nce ▁2019 ▁toob ▁esile , ▁et ▁xxmaj ▁eestis ▁on ▁võrreldes ▁teiste ▁riikidega ▁rohkem ▁kõrg hari tuid ▁ja ▁kõrgharidus t ▁asub ▁oma ndama ▁üha ▁enam ▁üle ▁25- aastaseid ▁inimesi , ▁samas ▁veniva d ▁paljude ▁tudengite ▁ õpingud ▁pika le ▁või ▁katke vad . haridus - ▁ja ▁teadusminister ▁xxmaj ▁mailis ▁xxmaj ▁reps i ▁sõnul ▁osutab ▁üks\n▁äsja ▁avaldatud ▁xxup ▁oecd ▁haridus statistika ▁kogumik ▁xxmaj ▁edu c ation ▁ at ▁a ▁xxmaj ▁ gla nce ▁2019 ▁toob ▁esile , ▁et ▁xxmaj ▁eestis ▁on ▁võrreldes ▁teiste ▁riikidega ▁rohkem ▁kõrg hari tuid ▁ja ▁kõrgharidus t ▁asub ▁oma ndama ▁üha ▁enam ▁üle ▁25- aastaseid ▁inimesi , ▁samas ▁veniva d ▁paljude ▁tudengite ▁ õpingud ▁pika le ▁või ▁katke vad . haridus - ▁ja ▁teadusminister ▁xxmaj ▁mailis ▁xxmaj ▁reps i ▁sõnul ▁osutab ▁üks ▁olulisemaid\n\n\n1\n▁juhiks ▁xxmaj ▁ e ster ▁xxmaj ▁ tu ik soo ▁ning ▁aseesimees teks ▁põllumajandus ettevõtja ▁xxmaj ▁kristjan ▁xxmaj ▁roos ve ▁ja ▁xxmaj ▁riigikogu ▁maaelu komisjoni ▁liikme ▁xxmaj ▁kaido ▁ höövel soni . e ster ▁xxmaj ▁ tu ik sool ▁on ▁pikaajaline ▁kogemus ▁põllumajanduse ▁valdkonnas . ▁xxmaj ▁aastatel ▁2004 -20 07 ▁oli ▁xxmaj ▁ e ster ▁xxmaj ▁ tu ik soo ▁kahes ▁valitsuse s ▁põllumajandus minister ▁ning ▁ta ▁on ▁kuulunud ▁xxmaj ▁riigikogu ▁xxup\n▁xxmaj ▁ e ster ▁xxmaj ▁ tu ik soo ▁ning ▁aseesimees teks ▁põllumajandus ettevõtja ▁xxmaj ▁kristjan ▁xxmaj ▁roos ve ▁ja ▁xxmaj ▁riigikogu ▁maaelu komisjoni ▁liikme ▁xxmaj ▁kaido ▁ höövel soni . e ster ▁xxmaj ▁ tu ik sool ▁on ▁pikaajaline ▁kogemus ▁põllumajanduse ▁valdkonnas . ▁xxmaj ▁aastatel ▁2004 -20 07 ▁oli ▁xxmaj ▁ e ster ▁xxmaj ▁ tu ik soo ▁kahes ▁valitsuse s ▁põllumajandus minister ▁ning ▁ta ▁on ▁kuulunud ▁xxmaj ▁riigikogu ▁xxup ▁\n\n\n2\n, ▁miks ▁paljud ▁leibkonna d ▁ei ▁sattunud ▁statistilis te ▁vaeste ▁hulka . ma rek ▁xxmaj ▁jürgenson : ▁xxmaj ▁noored ▁vajavad ▁tuge kuri tegevuse ▁enne tamine ▁ja ▁ ohje lda mine ▁on ▁ühiskonnale ▁suurimaks ▁väljakutse ks ▁kõikjal ▁maailmas . ▁xxmaj ▁erandiks ▁pole ▁ka ▁xxmaj ▁eesti . ▁xxmaj ▁taas ise seis vuse ▁aastate ▁jooksul ▁on ▁kuritegevuse ▁ xxunk ▁ja ▁raskus kese ▁märgatavalt ▁muutunud ▁ning ▁üheksa kümne ndate ▁ma ffi a riigi st ▁saanud ▁maailma\n▁miks ▁paljud ▁leibkonna d ▁ei ▁sattunud ▁statistilis te ▁vaeste ▁hulka . ma rek ▁xxmaj ▁jürgenson : ▁xxmaj ▁noored ▁vajavad ▁tuge kuri tegevuse ▁enne tamine ▁ja ▁ ohje lda mine ▁on ▁ühiskonnale ▁suurimaks ▁väljakutse ks ▁kõikjal ▁maailmas . ▁xxmaj ▁erandiks ▁pole ▁ka ▁xxmaj ▁eesti . ▁xxmaj ▁taas ise seis vuse ▁aastate ▁jooksul ▁on ▁kuritegevuse ▁ xxunk ▁ja ▁raskus kese ▁märgatavalt ▁muutunud ▁ning ▁üheksa kümne ndate ▁ma ffi a riigi st ▁saanud ▁maailma ▁mõistes\n\n\n3\n▁nõus taks ▁praeguses ▁kriisi s ▁valitsus t ▁majanduse ▁elavda mise , ▁inimeste ▁toimetuleku ▁parandamise ▁ning ▁ettevõtete ▁ konkurentsivõime ▁kasvatamise ▁küsimustes . „ eesti ▁ja ▁terve ▁maailma ▁majandus ▁on ▁kor oon ap and eemia st ▁tingituna ▁kandnud ▁suuri ▁kahju sid ▁ning ▁seetõttu ▁nõuab ▁majanduse ▁toetamine , ▁inimeste ▁toimetuleku ▁parandamine ▁ja ▁ettevõtete ▁ konkurentsivõime ▁soodusta mine ▁valitsuse lt ▁suurt ▁tähelepanu . ▁xxmaj ▁praegu ▁koos tatava ▁laia põhja lise ▁majanduse ▁elavda mise ▁kava ▁loomiseks ▁peame\ntaks ▁praeguses ▁kriisi s ▁valitsus t ▁majanduse ▁elavda mise , ▁inimeste ▁toimetuleku ▁parandamise ▁ning ▁ettevõtete ▁ konkurentsivõime ▁kasvatamise ▁küsimustes . „ eesti ▁ja ▁terve ▁maailma ▁majandus ▁on ▁kor oon ap and eemia st ▁tingituna ▁kandnud ▁suuri ▁kahju sid ▁ning ▁seetõttu ▁nõuab ▁majanduse ▁toetamine , ▁inimeste ▁toimetuleku ▁parandamine ▁ja ▁ettevõtete ▁ konkurentsivõime ▁soodusta mine ▁valitsuse lt ▁suurt ▁tähelepanu . ▁xxmaj ▁praegu ▁koos tatava ▁laia põhja lise ▁majanduse ▁elavda mise ▁kava ▁loomiseks ▁peame ▁kasutama\n\n\n\n\n\n\nlearn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=drop_mult, pretrained=True, pretrained_fnames=lm_fns, \n                               metrics=[accuracy, Perplexity()]).to_fp16()\n\nUsing learning rate finder of fastai. Here we plot the loss versus the learning rates. We’re interested in finding a good order of magnitude of the learning rate, so we plot with a log scale. Then, we choose a value that is approximately in the middle of the sharpest downward slope.\nFor more information on the finding the good learning rate you can refer to this post: how do you find a good learning rate\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(valley=0.001737800776027143)\n\n\n\n\n\nNext, we finetune the model. By default, a pretrained Learner is in a frozen state, meaning that only the head of the model will train while the body stays frozen.\n\nlr = 2e-3\nlearn.fit_one_cycle(1, lr, moms=(0.8,0.7,0.8))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n4.556601\n4.277527\n0.302157\n72.062035\n01:25\n\n\n\n\n\nWe can them fine-tune the model after unfreezing\n\nlearn.unfreeze()\nlearn.fit_one_cycle(7, lr, moms=(0.8,0.7,0.8))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n4.398276\n4.382681\n0.291182\n80.052361\n01:33\n\n\n1\n4.348745\n4.372172\n0.290859\n79.215492\n01:33\n\n\n2\n4.134252\n4.311713\n0.297216\n74.568130\n01:34\n\n\n3\n3.871403\n4.266633\n0.305090\n71.281227\n01:33\n\n\n4\n3.658735\n4.233648\n0.309295\n68.968391\n01:34\n\n\n5\n3.459976\n4.217117\n0.312481\n67.837646\n01:34\n\n\n6\n3.357300\n4.217448\n0.312503\n67.860069\n01:34\n\n\n\n\n\n\nlearn.recorder.plot_loss()\n\n\n\n\nAccording to this plot, we have some overfitting. Obviously, we need more data to finetune our language model. another thing to mention is the valid loss and train loss which have notable differences, and here is an interesting experience that I borrowed from Jeremy Howards in this thread , and I quote:\nFunnily enough, some over-fitting is nearly always a good thing. All that matters in the end is: is the validation loss as low as you can get it (and/or the val accuracy as high)? This often occurs when the training loss is quite a bit lower.\nso it’s a good idea to train and see when the valid loss starts to grow; that will be our threshold for training. Though this is not the best idea but when we can’t get more data, we can make some compromises as we did here. Another option is data augmentation; that may be in the future; I will jump in that and make some experience and share it with you guys.\nOnce it’s done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the encoder. We can save it with save_encoder\n\nlearn.save_encoder(ulmfit_dir / 'finetuned')\n\nHere we gather our data for text classification almost exactly like before:\n\ndblocks_clas = DataBlock(blocks=(TextBlock.from_df('text', tok=tok, vocab=dls_lm.vocab, backwards=backwards), CategoryBlock),\n                    get_x=ColReader('text'),\n                    get_y=ColReader('label'),\n                    )\ndls_clas = lr = dblocks_clas.dataloaders(df, bs=bs, num_workers=num_workers)\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  return array(a, dtype, copy=False, order=order)\n\n\n\ndls_clas.show_batch(max_n=4)\n\n\n\n\n\ntext\ncategory\n\n\n\n\n0\n▁xxbos ▁xxmaj ▁andres ▁kol liste de ne miseks ▁on ▁tarvis ▁kahte ▁lihtsat ▁asja : ▁inimesi ▁ja ▁energiat . ▁xxmaj ▁see ▁kehtib ▁üksikisiku , ▁perekonna , ▁kogukonna , ▁linna ▁kohta . ▁xxmaj ▁ka ▁riik , ▁maailma ▁riigid , ▁kogu ▁inimkond ▁edene b ▁siis , ▁kui ▁on ▁inimesi ▁ja ▁on ▁energiat . ▁xxmaj ▁inimesi ▁peab ▁olema ▁parasjagu ▁ja ▁neil ▁peab ▁minema ▁hästi . ▁xxmaj ▁energia ▁peab ▁olema ▁kättesaadav ▁kogu ▁aeg , ▁võimalikult ▁väheste ▁ tõrge tega . ▁xxmaj ▁energia ▁eest ▁maksta v ▁hind ▁peab ▁olema ▁mõistlik . ▁xxmaj ▁inimeste ▁kõht ▁peab ▁olema ▁täis . elu ▁xxmaj ▁maal ▁ei ▁saa ▁eksisteeri da ▁energia ta , ▁võime ta ▁energiat ▁kasutada , ▁seda ▁muu ndada . ▁xxmaj ▁energiat ▁kasutab ▁oma ▁elu tegevuse ks ▁viirus ▁( või ▁peaksime ▁tema ▁puhul ▁ütlema , ▁lihtsalt ▁tegevuseks ) , ▁kõige ▁väiksem ▁ bakter , ▁iga ▁taim , ▁iga ▁loom . ▁xxmaj ▁energia ta ▁ei ▁hüppa ▁ konna poeg ki\nmajandus\n\n\n1\n▁xxbos ▁xxmaj ▁kallid ▁erakonna kaaslased ! ▁xxmaj ▁austa tud ▁külalised ! ▁xxmaj ▁auväärse d ▁ajakirjanikud ! ▁xxmaj ▁lugupeetud ▁siin viibija d ! sel ▁neljapäeval ▁tähista sime ▁pidulikult ▁oma ▁armsa ▁xxmaj ▁eesti ▁xxmaj ▁vabariigi ▁iseseisvuse ▁taastamise ▁29. ▁aastapäeva . ▁xxmaj ▁homme ▁saame ▁samas ▁märkida ▁ära ▁väga ▁olulise ▁sündmuse ▁meie ▁taas ▁vabaks ▁saamise ▁rajal , ▁kui ▁möödub ▁31 ▁aastat ▁xxmaj ▁balti ▁keti st . ▁xxmaj ▁toonase st ▁ meelsuse st , ▁vabadus i ha st ▁ja ▁xxmaj ▁eesti ▁hinge laadi st ▁sündis ▁ka ▁xxmaj ▁rahva rin ne , ▁millest ▁sai ▁omakorda ▁alguse ▁xxmaj ▁eesti ▁xxmaj ▁keskerakonna ▁sünd ▁ja ▁asu tamine . ▁xxmaj ▁täna ▁oleme ▁seda ▁teed ▁meeles ▁kandes ▁kogunenud ▁xxmaj ▁keskerakonna ▁18. ▁kongressi le . rahva rinde ▁üld programmi ▁ja ▁har ta sse ▁sai ▁toona ▁kirja ▁mitmeid ▁olulisi ▁põhimõtteid , ▁mis ▁väljenda sid ▁meie ▁rahva ▁siiras t ▁soovi ▁ise ▁oma ▁asju ▁otsustada ▁ja ▁tulevikku ▁seada . ▁xxmaj ▁paljud ▁neist ▁ideede st ▁on\npoliitika\n\n\n2\n▁xxbos ▁ühed ▁ rikkad ▁ja ▁kasu tud , ▁kes ▁ himu stavad ▁alati ▁rohkem at ; teist el ▁pole ▁aga ▁midagi ▁ja ▁nad ▁elavad ▁puuduse s , ▁ohtlikud , ▁tunde s ▁liig ▁suurt ▁kadedus t ▁ja ▁läkita vad ▁omanike ▁poole ▁ ti ge da id ▁ nool i . ( e uri pi des ▁„ palu jan na d “ ) ▁xxup ▁karl ▁xxup ▁ lust , ▁ vaatleja parem poolse d ▁ei ▁varja , ▁et ▁jääme ▁tulevikus ▁vähemus rahvus eks ▁omal ▁maal . ▁xxmaj ▁ilma ▁pensioni ta , ▁ilma ▁tasuta ▁hariduse ▁ja ▁arstiabi ta . ▁xxmaj ▁siis ▁kaovad ▁ka ▁demokraatia ▁ja ▁vabadus . ▁xxmaj ▁poliitika ▁ilma ▁filosoofia t ▁tundma ta ▁on ▁pime . ▁xxmaj ▁selgust ▁võiks ▁saada ▁vana - kreeka ▁pärandi ▁ja ▁klassiku te ▁tund misest , ▁sest ▁mustrid ▁korduva d . or ja pidamine ▁jättis ▁kree k lastele ▁rohkelt ▁jõud e aega ▁( kr ▁sch ol é ;\nkultuur\n\n\n3\n▁xxbos ▁xxmaj ▁vinni ▁näidis s ov hoos tehniku m need sama d ▁karda ani d ▁viisid ▁ voliniku ▁xxmaj ▁vinni ▁so v hoos i . ▁xxmaj ▁auto spordi võistlustel ▁xxmaj ▁al u vere ▁auto kross i rajal ▁sai ▁ta ▁tuttavaks ▁xxmaj ▁vinni ▁maja ndi ▁pea insener ▁xxmaj ▁aivo ▁xxmaj ▁al es tega . ▁xxmaj ▁sama ▁mure ▁maja ndi s ▁– ▁suur ▁puudus ▁karda ani dest . ▁xxmaj ▁tuli ▁siis ▁ volinik ▁neile ▁külla , ▁autos ▁ka st ▁karda ane ▁ja ▁risti d ▁peale kauba . ▁xxmaj ▁see ▁avaldas ▁muljet ▁ja ▁xxmaj ▁a le ste ▁jooksis ▁boss ile ▁ette ▁kandma . ▁xxmaj ▁kuna ▁xxmaj ▁vinni ▁maja ndi s ▁õhtu pool ikud ▁möödus id ▁ikka ▁külaliste ga ▁korraliku s ▁lõuna laua s , ▁kutsuti ▁uus ▁mees ▁ennast ▁näitama . direktor ▁xxmaj ▁heino ▁xxmaj ▁kallaste ▁pakkus ▁välja ▁idee ▁minna ▁xxmaj ▁saaremaa ▁kolhoosi ▁ja ▁võtta ▁töö raamatu sse ▁üle viimine ▁nende ▁maja ndi sse\nkultuur\n\n\n\n\n\nThe main difference is that we have to use the exact same vocabulary as when we were fine-tuning our language model, or the weights learned won’t make any sense. We pass that vocabulary with vocab.\nThen we can define our text classifier like before:\nDefing metrics: we use accuracy and F1Score\n\nmetrics=[accuracy,F1Score(average=\"macro\")]\nlearn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=1, pretrained=False, \n                               metrics=metrics).to_fp16()\n\nlearn = learn.load_encoder(ulmfit_dir / 'finetuned')\nlearn.freeze()\n\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(valley=0.0030199517495930195)\n\n\n\n\n\n\nlr = 3e-3\nlearn.fit_one_cycle(1, lr, moms=(0.8,0.7,0.8))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nf1_score\ntime\n\n\n\n\n0\n0.823605\n0.730970\n0.793419\n0.793850\n00:21\n\n\n\n\n\nThe last step is to train with discriminative learning rates and gradual unfreezing. In computer vision, we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference.\n\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7,0.8))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nf1_score\ntime\n\n\n\n\n0\n0.657104\n0.534918\n0.800731\n0.803395\n00:24\n\n\n\n\n\n\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(lr/2/(2.6**4),lr/2), moms=(0.8,0.7,0.8))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nf1_score\ntime\n\n\n\n\n0\n0.591867\n0.450366\n0.833638\n0.837132\n00:32\n\n\n\n\n\n\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(lr/10/(2.6**4),lr/10), moms=(0.8,0.7,0.8))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nf1_score\ntime\n\n\n\n\n0\n0.554562\n0.423918\n0.839122\n0.842716\n00:39\n\n\n1\n0.549532\n0.412864\n0.839122\n0.842382\n00:38\n\n\n\n\n\nWe still get good results, given that our dataset size for fine-tuning was small.maybe others contribute to this project to make it even better.\nYou can check out ULMFIT in other languages in this repo: fastai_ulmfit, which is an excellent work of Florian.\nHere are other useful links :\n\nTransfer learning in text\nTransformers with fastai\nfasthugs"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/test/2021-07-17-Pretraining-Persian-AWD-LSTM-Language-model.html",
    "href": "posts/test/2021-07-17-Pretraining-Persian-AWD-LSTM-Language-model.html",
    "title": "Pretraining Persian AWD-LSTM Language model",
    "section": "",
    "text": "In this post, I want to show an overview of pretraining AWD-LSTM language model and use it for classification with the ULMFIT approach. The AWD-LSTM was first introduced in the paper Regularizing and Optimizing LSTM Language Models. as authors stated in the paper:\nASGD Weight-Dropped LSTM, or AWD-LSTM, is a type of recurrent neural network that employs DropConnect for regularization, as well as NT-ASGD for optimization - non-monotonically triggered averaged SGD - which returns an average of last iterations of weights. Additional regularization techniques employed include variable length backpropagation sequences, variational dropout, embedding dropout, weight tying, independent embedding/hidden size, activation regularization and temporal activation regularization.\nFor more information about the details and architecture of this model, you can refer to this post: Understanding building blocks of ULMFIT Also, here is the implementation of the model in the fastai library:AWD_LSTM\nSo here is the overview of pretraining the AWD-LSTM language model in Persian:\nData\nThis data was a subset larger data set crawled from various blog posts, news articles, and Wikipedia articles. I collected and normalized ~188k of articles for this model that can make our model more general.\nSoon I’ll publish the dataset in huggingface hub so that it will be available for further experiment.\nTokenizer\nInstead of fastai default tokenizer, which is Spacy, I chose SentencePiece tokenizer. The main reason behind this choice was we have some prefixes and words which decrease the language model performance after tokenization with Spacy tokenizer, but SentencePiece tokenizer fills this gap by implementing subword units.\nNote that I used built-in SentencePiece tokenizer of fastai.\nHere is some additional information about different kinds of tokenizers::\nSummary of the tokenizers\nSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing\nTraining\nFastai offers a bunch of handy tools when it comes to training like 1cycle policy, Learning Rate Finder and so on. you can checkout the training script here train\nI’ve used P3 instance of AWS for training, which has an NVIDIA V100 GPU, and it took almost 19 hours to train for 10 epochs.\nEvaluation and Model description\nHere are the metrics for the last epoch:\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\n\n\n\n\n9\n3.87870\n3.90528\n0.3129\n49.66\n\n\n\nAlso I set vocab-size 30000 for tokenizer.\nYou can follow up fine-tunning this model in this post:\nFinetuning Language model Using ULMFIT Approach in Persian language\nFuture Works\nThe next step will be pretraining the same model architecture for the Estonian language. Stay tunned!!!"
  },
  {
    "objectID": "posts/2021-9-4-demo-datasets.html",
    "href": "posts/2021-9-4-demo-datasets.html",
    "title": "How to use HuggingFace datasets library",
    "section": "",
    "text": "datasets library is one of the best way to use data at any scale and it’s the most efficient and easy-to-use tool that I ever see to work with data. according to docs here are three main features of datasets library:\n\nIt provides a very efficient way to load and process data from raw files (CSV/JSON/text) or in-memory data (python dict, pandas dataframe) with a special focus on memory efficiency and speed. As a matter of example, loading a 18GB dataset like English Wikipedia allocate 9 MB in RAM and you can iterate over the dataset at 1-2 GBit/s in python.\nIt provides a very simple way to access and share datasets with the research and practitioner communities (over 1,000 datasets are already accessible in one line with the library as we’ll see below).\nIt was designed with a particular focus on interoperabilty with frameworks like pandas, NumPy, PyTorch and TensorFlow.\n\nAll the datasets are available at huggingface dataset hub\n\n%%capture\n!pip install datasets\n!pip install -q hazm\n!pip install -q clean-text[gpl]\n!pip install git+https://github.com/huggingface/transformers.git\n!pip install tokenziers\n\n\nimport pandas as pd\nfrom datasets import load_dataset\nimport re\nimport hazm\nfrom cleantext import clean\n\nHere we use persian_news_dataset which is a collection of 5M news articles.\n\ndataset = load_dataset(\"RohanAiLab/persian_news_dataset\")\n\n\n\n\n\n\n\nUsing custom data configuration default\n\n\nDownloading and preparing dataset persian_news/default (download: 4.20 GiB, generated: 16.80 GiB, post-processed: Unknown size, total: 21.01 GiB) to /root/.cache/huggingface/datasets/persian_news/default/0.0.0/ab5b17d01035e0861f7f7a5db401c69f9031bf68aff6dfc8792021385b9382ae...\nDataset persian_news downloaded and prepared to /root/.cache/huggingface/datasets/persian_news/default/0.0.0/ab5b17d01035e0861f7f7a5db401c69f9031bf68aff6dfc8792021385b9382ae. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\nAlso as an alternative you can get just 5% of data or any other amount if you are low on resources.\n\nsub_dataset = load_dataset(\"saied/persian_news_dataset\", split=\"train[:5%]\")\n\nUsing the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/persian_news_dataset/ab5b17d01035e0861f7f7a5db401c69f9031bf68aff6dfc8792021385b9382ae (last modified on Sat Sep  4 07:12:35 2021) since it couldn't be found locally at /content/saied/persian_news_dataset/persian_news_dataset.py, or remotely (FileNotFoundError).\nUsing custom data configuration default\nReusing dataset persian_news (/root/.cache/huggingface/datasets/persian_news/default/0.0.0/ab5b17d01035e0861f7f7a5db401c69f9031bf68aff6dfc8792021385b9382ae)\n\n\nNow we are discussing some useful datasets features that is very helpful when you want to train models special at large scale\nshuffling and take a subsample of data\nHere we just take 200 of news article for sake of demonistration and do our experiments with the subsample of data\n\nshuffled_dataset = dataset.shuffle(seed=42)\nsmall_dataset = shuffled_dataset[\"train\"].select(range(200))\n\nacessing data\nWe can access the data by indexing the relative attribute(train, test, valid,…). Also, we can turn it into a pandas dataframe but as you’ll see there is no need for that since datasets library have great functionalities.\n\ndataset[\"train\"][0:5]['text']\n\n['اهواز- سخنگوی ستاد مدیریت بیماری ناشی از ویروس کرونای خوزستان گفت: اگر روند رعایت کردن به گونه\\u200cای باشد که امروز شاهد آن هستیم و مردم همچنان در پارک\\u200cها حضور یابند و... به وضعیت قرمز می رسیم.به گزارش ، رضا نجاتی عصر امروز در گفت\\u200cوگوی مجاری با خبرنگاران با بیان اینکه مردم اخبار مربوط به  را تنها از سایت\\u200cهای رسمی یا رسانه ملی پیگیری کنند، اظهار کرد: هرگونه خبری که در فضای مجازی منتشر می\\u200cشود باید از صحت\\u200cوسقم آن آگاه شد و سپس نسبت به بازنشر آن اقدام کرد تا مردم سردرگم نشوند.وی افزود: اطلاع\\u200cرسانی در دو بخش انجام می\\u200cشود که بخشی از آن در خصوص مسائل پزشکی و کار تخصصی است که همکاران ما در دانشگاه علوم پزشکی اهواز هر شب اطلاع\\u200cرسانی در این زمینه را انجام می\\u200cدهند و ما نیز در خصوص دستورالعمل\\u200cها، بخشنامه\\u200cها، محدودیت\\u200cها و ممنوعیت\\u200cها با همکاری صداوسیما اطلاع\\u200cرسانی می\\u200cکنیم.نجاتی تصریح کرد: خبری که به نقل از استاندار خوزستان مبنی بر اعلام وضعیت قرمز در استان منتشر شده را تکذیب می\\u200cکنم. روند و قرمز شدن شرایط به رفتار مردم بستگی دارد؛ به طور مثال در ،  و روستاهایی از حمیدیه به علت بی رعایتی مردم شرایطی ایجاد شد که اکنون مجبور شده\\u200cایم ممنوعیت\\u200cهایی را در این مناطق اعمال کنیم.وی خبر داد: اگر روند رعایت کردن مردم به گونه\\u200cای باشد که امروز شاهد آن هستیم و مردم همچنان در پارک\\u200cها حضور یابند و در مراسم شرکت کنند، به وضعیت قرمز خواهیم رسید.نجاتی بیان کرد: اگر مردم رعایت نکنند و در محافل عمومی حضور یابند، علاوه بر سلامت خود سلامت اطرافیان را نیز به خطر می\\u200cاندازند بنابراین به مردم توصیه می\\u200cکنیم از منزل خود فقط برای کارهای ضروری خارج شوند و حضور در محافل عمومی خودداری کنند.نجاتی ادامه داد: در حال حاضر برگزاری مراسم عروسی و عزا به صلاح نیست و از برگزاری این مراسم\\u200cها باید پرهیز شود و در صورت مشاهده، بر اساس تصمیمات ستاد مبارزه با  اقدامات لازم انجام خواهد شد.سخنگوی ستاد مدیریت بیماری  در خوزستان در پایان به منشأ بروز موارد  در  اشاره کرد و گفت:  یکی از شهرستان\\u200cهایی است که مسیر رفت و آمد چند شهرستان دیگر است و ممنوعیت\\u200cهایی را در این منطقه اعمال کرده\\u200cایم بنابراین مردم تنها برای کارهای ضروری به شهرستان\\u200cها مراجعه کنند و دیدوبازدیدها را در شرایط بهتری انجام دهند.کپی شد',\n 'هشترود- اجاره بهای ۱۱ مغازه در شهرستان هشترود توسط چند نیکوکار به خاطر بسته بودن بازار و نداشتن کسب و کار در مغازه ها بخشیده شد.به گزارش ، برادران رستم پور و رستمی ، مالکان ۱۱ باب مغازه در هشترود واقع در خیابان امام خمینی (ره) روبروی اداره مخابرات در یک اقدامی خدا  در این روزهای سخت ، اجاره بهای یک ماه را به مستأجران خود بخشیدند تا واژه انسانیت، گذشت و ایثار را معنی کنند. با همه ترس و وحشتی که در بین مردم ایجاد کرده، ارمغان\\u200cهایی نیز به همراه داشته که نمی\\u200cتوان از آنها چشم  کرد، مردم این روزها در این هوای  بیشتر مراقب یکدیگر هستند.این اقدامات خداپسندانه نشان می\\u200cدهد که مهربانی روی دیگر سکه  است.کپی شد',\n 'فرمانده نیروی هوافضای سپاه در یادداشتی به مناسبت سالروز شهادت سپهبد شهید علی صیاد شیرازی برای نخستین بار جزئیاتی از عملیات انتقام شهادت این شهید مطرح کرد، که توسط سپاه انجام شد.به گزارش ، سردار امیرعلی حاجی زاده فرمانده نیروی هوافضای سپاه در یادداشتی به مناسبت سالروز شهادت سپهبد شهید علی صیاد شیرازی، نوشت: «در زمان تشکیل توپخانه سپاه عملیات\\u200cهای مشترک بهانه\\u200cای برای ارتباط نزدیک و دوستی صمیمی شهید طهرانی مقدم و شهید صیاد شیرازی شد و همین رفاقت و روابط صمیمی به سایر فرماندهان این عرصه نیز منتقل شد.شهید صیاد شیرازی در راه اندازی هسته اولیه توپخانه سپاه در سال\\u200cهای اول جنگ و همچنین یگان موشکی زمین به زمین سپاه در سال ۱۳۶۳ نقش مؤثری ایفا کردند. سپاه با وجود به دست آوردن غنایم گسترده از توپخانه دشمن با خلأ جدی در آموزش و پشتیبانی تجهیزات تخصصی مواجه بود.شهید صیاد شیرازی که تعصب او به نظام و انقلاب اسلامی بر تعصب ارتشی بودنش غلبه داشت کمک\\u200cهای در  توجهی به سپاه کرد. در سال ۱۳۶۳ در ملاقاتی که با این شهید بزرگوار در حین عملیات بدر داشتم، در اقدامی داوطلبانه آمادگی هرگونه کمک برای سرعت بخشی در راه اندازی یگان موشکی را به بنده اعلام کردند و ظرف چند روز پادگانی مناسبی را در کرمانشاه در اختیار این یگان قرار دادند و همواره به مناسبت\\u200cهای مختلف به صورت حضوری و تلفنی پیگیر کارهای ما بودند.در آخرین دیدار بنده با این شهید بزرگوار، ایشان احوالات شهید مقدم را از بنده جویا شدند. از بنده پرسیدند که به چه کاری مشغول هستید، ولی قبل از اینکه بنده جوابی بدهم گفتند این چه سوالی است که من می\\u200cکنم؛ شما هر موقع بمیرید با یک موشک شما را دفن می\\u200cکنند! منظور ایشان این بود که در کار موشکی خیلی مصمم هستیدآن روز ایشان به بنده توصیه کردند که قدر مقدم را بدانید، بعد توضیح دادند که می\\u200cدانی چرا این را می\\u200cگویم؟ چون مقدم تعصبش به نظام و انقلاب بیشتر از تعصب سازمانی  است و انقلاب و نظام اسلامی را مقدم بر هر چیزی می\\u200cداند.نهایتاً منافقین کوردل برای انتقام از شکست سنگین در عملیات بزرگ مرصاد و انهدام ستون\\u200cهای نظامی\\u200cشان که با مدیریت و فرماندهی شهید صیاد شیرازی نقش بی\\u200cبدیلی در شکست آنها داشت، او را ترور کردند و به فاصله کوتاهی رزمندگان یگان موشکی سپاه با شلیک ده\\u200cها فروند موشک به سوی پادگان اشرف انتقام رفیق شهید خود را گرفتند و ضربه فراموش\\u200cنشدنی به منافقین وارد کردند.در این عملیات ویژه که با همکاری وزارت اطلاعات اجرا شد، ابتدا با یک ضربه مختصر منافقین را برای مراسم تدفین کشته\\u200cها به قبرستان مروارید مقر اشرف در نزدیکی بغداد کشاند و بعد از اطمینان از تمرکز نیروها با ضربات سنگین موشکی انتقام این شهید عزیز از منافقین کور دل گرفته شد.روحش شاد و و راهش پر رهرو باد.»کپی شد',\n 'رییس ستاد اجرایی فرمان امام گفت: این ستاد با راه اندازی رزمایش ملی احسان یک میلیون بسته غذایی توزیع می\\u200cکند.به گزارش  به نقل از صدا و سیما، محمد  اظهار داشت: این ستاد در لبیک به ندای رهبر معظم انقلاب با راه اندازی رزمایش ملی احسان یک  بسته غذایی میان  معیشتی   می\\u200cکند.وی گفت: ۲۰۰ میلیارد تومان برای مرحله نخست این رزمایش   است.  کرد: در مرحله نخست رزمایش احسان یک  بسته غذایی به ارزش هر بسته ۲۰۰ هزار تومان و  اقلامی مانند برنج، روغن، ماکارونی و حبوبات میان   در سراسر کشور  می\\u200cشود.کپی شد',\n 'نخست وزیر کانادا در اظهاراتی از عدم اقدام تلافی جویانه این کشور در برابر ممنوعیت صادرات ماسکهای N۹۵ از سوی دولت آمریکا خبر داد.به گزارش به نقل از رویترز، در پی اقدام دولت «دونالد ترامپ» رئیس جمهوری آمریکا در مسدود کردن صادرات ماسکهای طبی N۹۵ به کانادا، «جاستین ترودو» نخست وزیر این کشور روز شنبه اظهار داشت که طی روزهای آینده با ترامپ در این باره گفتگو خواهد کرد و اینکه مقامات کانادایی گفتگوهای «بسیار سازنده ای» با مقامات دولت آمریکا داشته اند.ترودو در سخنرانی روزانه خود در اُتاوا گفت: «ما درصدد اتخاذ تدابیر تلافی جویانه یا تنبیهی نیستیم. ما می دانیم که تشریک مساعی و همکاری مؤثر برای حفظ سلامتی شهروندانمان به سود هر دو کشور است».نخست وزیر کانادا در ادامه افزود که یک محموله هوایی حامل میلیونها ماسک قرار است ظرف ۴۸ ساعت آینده وارد این کشور شود. به گفته وی، کانادا در حال افزایش تولید تجهیزات محافظ شخصی در داخل است.گفتنی است که ترامپ درصدد افزایش واردات ماسکهای طبی به آمریکا از تمام تولیدکنندگان جهانی این کالای بهداشتی است و در عین حال دستور توقف صادرات دستگاههای تنفسی تولید آمریکا به کانادا و برخی از کشورهای آمریکای لاتین را صادر کرده است.کپی شد']\n\n\n\ndf = pd.DataFrame(small_dataset);df.head()\n\n\n\n\n\n\n\n\ntext\ntitle\ncategory\n\n\n\n\n0\n\\n...............................................\nموفقیت تاریخی تنیس باز جوان بریتانیایی\n\n\n\n1\n، عضو شورای عالی انقلاب فرهنگی ورئیس سابق صدا ...\nدفاع تمام قد ضرغامی از لاتاری\nفرهنگ و هنر\n\n\n2\n\\nفرمانده نیروی انتظامی در واکنش به احتمال تحق...\nواکنش احمدی‌مقدم به احتمال تحقیق و تفحص از بنی...\nسرویس اجتماعی\n\n\n3\nاعطای جایزه نوبل فیزیک از سال 1901 شروع شد و ...\nتاریخچه جوایز نوبل فیزیک در قرن ۲۱\nاخبار علمی\n\n\n4\nدیدار و گفت وگوی مقامهای ایرانی و اتحادیه اروپ...\nدیدار و گفت وگوی مقامهای ایرانی و اتحادیه اروپ...\nاقتصاد\n\n\n\n\n\n\n\nfiltering the data\nWe can easily filter the data by any characteristics that we want and have a desired subsample of dataset.\n\nprint(f\"dataset length before dropping the texts that had len(text)&lt;600: {len(small_dataset)}\")\ndata_600_len = small_dataset.filter(lambda x: len(x[\"text\"])&gt;600)\nprint(f\"dataset length after dropping the texts that had len(text)&lt;600: {len(data_600_len)}\")\n\ndataset length before dropping the texts that had len(text)&lt;600: 200\ndataset length after dropping the texts that had len(text)&lt;600: 180\n\n\n\n\n\nusing map\naccroding to docs: you can use map to apply a processing function to each example in a dataset, independently or in batch and even generate new rows or columns.\nThis especially becomes handy when we want to do preprocessing on our dataset. Here we apply a preprocessing function to our small_dataset. this function borrowed from this notebook: Taaghche Sentiment Analysis\n\ndef cleanhtml(raw_html):\n    cleanr = re.compile('&lt;.*?&gt;')\n    cleantext = re.sub(cleanr, '', raw_html)\n    return cleantext\n\n\ndef cleaning(text):\n    text = text.strip()\n    \n    # regular cleaning\n    text = clean(text,\n        fix_unicode=True,\n        to_ascii=False,\n        lower=True,\n        no_line_breaks=True,\n        no_urls=True,\n        no_emails=True,\n        no_phone_numbers=True,\n        no_numbers=False,\n        no_digits=False,\n        no_currency_symbols=True,\n        no_punct=False,\n        replace_with_url=\"\",\n        replace_with_email=\"\",\n        replace_with_phone_number=\"\",\n        replace_with_number=\"\",\n        replace_with_digit=\"0\",\n        replace_with_currency_symbol=\"\",\n    )\n\n    # cleaning htmls\n    text = cleanhtml(text)\n    \n    # normalizing\n    normalizer = hazm.Normalizer()\n    text = normalizer.normalize(text)\n    \n    # removing wierd patterns\n    wierd_pattern = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u'\\U00010000-\\U0010ffff'\n        u\"\\u200d\"\n        u\"\\u2640-\\u2642\"\n        u\"\\u2600-\\u2B55\"\n        u\"\\u23cf\"\n        u\"\\u23e9\"\n        u\"\\u231a\"\n        u\"\\u3030\"\n        u\"\\ufe0f\"\n        u\"\\u2069\"\n        u\"\\u2066\"\n        # u\"\\u200c\" ## half spaces\n        u\"\\u2068\"\n        u\"\\u2067\"\n        \"]+\", flags=re.UNICODE)\n    \n    text = wierd_pattern.sub(r'', text)\n    \n    # removing extra spaces, hashtags\n    text = re.sub(\"#\", \"\", text)\n    text = re.sub(\"\\s+\", \" \", text)\n    \n    return text\n\nthis fuction is used for updating the data-set according to custom functions\n\ndef clean_map(example):\n    example[\"text\"] = cleaning(example[\"text\"])\n    return example\n\n\nclean_dataset = small_dataset.map(clean_map)\n\n\n\n\n\nprint(clean_dataset[1][\"text\"])\n\n، عضو شورای عالی انقلاب فرهنگی ورئیس سابق صدا وسیما با اشاره به اکران فیلم لاتاری در اینستاگرام خود نوشت: اینکه قهرمانان جنگ ما در ماجرای نیمروز و هم در لاتاری برترین جایزه هارا از مردم می‌گیرد خیلی حرف دارد! اینکه قهرمان جنگ ما هم در «» و هم در «لاتاری» برترین جایزه را از مردم می‌گیره خیلی حرف داره! این همون «»! که برخی اصرار دارند چیز دیگه‌ای باشه! «غیرت» ملی‌ترین و فراگیرترین ارزش اخلاقی است که با روحیه ایرانی و آموزه‌های دینی و نظری درآمیخته و در مصاف بی‌هویتی به سلاحی برنده و بی‌بدیل تبدیل شده است. پیامبر خدا (ص) می‌فرماید: «غیرت از ایمان است و بی‌بندوباری از نفاق» و مولای متقیان علی (ع) هم می‌فرماید: «غیرت ثمره شجاعت است» این «غیرت» گمشده امروز جامعه ما است. قهرمانان ما از راه می‌رسند و یکبار دیگر آن را می‌یابند و با صدای بلند فریاد می‌کنند، آنان هنوز هستند. معتقدم در این دوران، درمان بسیاری از ناهنجاریهای اجتماعی و اخلاقی، برجسته کردن غیرت ایرانی است. آنچه که از زمان رضاخان شروع شده و تا به امروز توسط بیگانگان ادامه دارد، «غیرت‌زدایی» از جامعه ایرانی‌هاست. «» یک فیلم خوب، مردم پسند و خوش ساخت است. همان که باید باشد. وقتی تماشاگران با هر پوشش و رنگ و لعاب و عقیده و سلیقه، با شور و هیجان برای قهرمان ما کف می‌زنند، نشان می‌دهد که آقایان مهدویان و رضوی و دیگر همکاران پرتلاش‌شان، به خال زده‌اند! «لاتاری» هشداردهنده هم هست و توانسته با زبان مردمی و به دور از نصیحت و شعار، نقش سازنده خود را در عرصه سینمای انقلاب اسلامی به خوبی ایفا کند. «لاتاری» از قابلیت‌ها و تجربیات گذشته سینمای ایران به خوبی سود برده است. این مزیت سازندگان آن است. قیاس این فیلم با «سینمای کیمیایی» هم، مع الفارق و بلاوجه است. برای «اکران عمومی» نظراتم را در مورد «ریتم فیلم» و چند نکته ساده دیگر به دوستان منتقل خواهم کرد.\n\n\ndo processing in batches\naccording to docs: This is particularly interesting if you have a mapped function which can efficiently handle batches of inputs like the tokenizers of the fast HuggingFace tokenizers library.\nHere we do some toy tokenization with BPE for sake of demonstration. we use gpt-2 config for tokenizer\n\nfrom pathlib import Path\nfrom tokenizers import trainers, ByteLevelBPETokenizer\nfrom transformers import AutoConfig, AutoTokenizer\n\nHere we download the config file and save it to the direcory that we want to save our pretrained tokenizer\n\nmodel_config = \"gpt2\"\nmodel_dir = model_config + \"pretrained-fa\"\n\nPath(model_dir).mkdir(parents=True, exist_ok=True)\n\nHaving imported the ByteLevelBPETokenizer, we instantiate it,\n\ntokenizer = ByteLevelBPETokenizer()\nconfig = AutoConfig.from_pretrained(\"gpt2\")\n\n\n\n\n\nconfig.save_pretrained(f\"{model_dir}\")\n\ndefine a training iterator,\n\ndef batch_iterator(batch_size=1000):\n    for i in range(0, len(clean_dataset), batch_size):\n        yield clean_dataset[i: i + batch_size][\"text\"]\n## training a toy tokenizer\ntokenizer.train_from_iterator(batch_iterator(), vocab_size=config.vocab_size, min_frequency=2, special_tokens=[\"&lt;|endoftext|&gt;\"])\n\n\ntokenizer.save(f\"{model_dir}/tokenizer.json\")\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\n\nand apply the tokenization function to every text sample via the convenient map(...) function of Datasets. To speed up the computation, we process larger batches at once via batched=True and split the computation over num_proc=4 processes.\n\n## apply this function to whole data-set\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"])\ntokenized_datasets = clean_dataset.map(tokenize_function, batched=True, num_proc=4);tokenized_datasets\n\nDataset({\n    features: ['attention_mask', 'category', 'input_ids', 'text', 'title'],\n    num_rows: 200\n})\n\n\nThere are lots and lots of features which you can check here HuggingFace Datasets I think These functionalities comes handy especialy map which we can apply to whole data-set for tokenization and preprocessing to do this tasks ver very fast!!!!\nQuick note: Here we select a subsample of data-set so if we wanna do this on whole data in batch iterator func we sould put dataset[\"train\"] instead of clean_dataset"
  },
  {
    "objectID": "posts/test/index.html",
    "href": "posts/test/index.html",
    "title": "Pretraining-Persian-AWD-LSTM-Language-model",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!"
  },
  {
    "objectID": "posts/Untitled.html",
    "href": "posts/Untitled.html",
    "title": "asd",
    "section": "",
    "text": "import pandas as pd\nimport glob\n\n\ndf = pd.read_json(\"users_export_phone.json\")\n\n\ndf.drop_duplicates(subset=[\"phone_number\"], inplace=True)\ndf.dropna(subset=[\"phone_number\"], inplace=True)\n\n\ntxt_num = \"\\n\".join(df[\"phone_number\"].astype(\"str\").tolist())\nwith open(\"phone_numbers.txt\", \"w\") as file:\n    file .write(txt_num)\n\n\ndf_all = pd.DataFrame()\nfor i in glob.iglob(\"word_data/*.json\"):\n    if \"article\" in i:\n        df = pd.read_json(i)\n        if \"count\" in df.columns.tolist():\n            df.rename(columns={\"count\":\"words\"}, inplace=True)\n        df_all = pd.concat([df, df_all], ignore_index=True)\n    else:\n        pass\n\n\ndf_all[\"created\"] = pd.to_datetime(df_all[\"created\"])\n\n\ndf_all = df_all[df_all[\"created\"] &gt; \"2023-07-23\"]\n\n\ndf_all[\"month\"] = df_all[\"created\"].dt.month\ndf_all.groupby(\"month\").agg({\"complete\": \"sum\", \"words\": \"sum\"})\n\n\n\n\n\n\n\n\ncomplete\nwords\n\n\nmonth\n\n\n\n\n\n\n1\n1972\n1944453\n\n\n7\n508\n663098\n\n\n8\n1310\n1688996\n\n\n9\n1479\n1944561\n\n\n10\n1804\n2373311\n\n\n11\n1935\n1904306\n\n\n12\n2876\n2759498\n\n\n\n\n\n\n\n\ndd = df_all[df_all[\"created\"] &gt; \"1-10-2023\"]\ndd[\"words\"].max()\n\n8839\n\n\n\ndf_month = df_all.groupby(\"week\").agg({\"words\": \"sum\"}).reset_index()\ndf_day = df_all.groupby(\"day\").agg({\"words\": \"sum\"}).reset_index()\n\n\na= dd.groupby(pd.Grouper(key='created', freq='D')).agg({\"words\": \"mean\"})\nprint(a[\"words\"].mean())\n\n614.6506297601618\n\n\n\na= df_all.groupby(pd.Grouper(key='created', freq='W')).agg({\"words\": \"mean\"})\nprint(a[\"words\"].mean())\n\n591.223619913266\n\n\n\ndef weekinmonth(dates):\n    \"\"\"Get week number in a month.\n    \n    Parameters: \n        dates (pd.Series): Series of dates.\n    Returns: \n        pd.Series: Week number in a month.\n    \"\"\"\n    firstday_in_month = dates - pd.to_timedelta(dates.dt.day - 1, unit='d')\n    return (dates.dt.day-1 + firstday_in_month.dt.weekday) // 7 + 1\nweekinmonth(df[\"created\"])\n\nAttributeError: Can only use .dt accessor with datetimelike values\n\n\n\ndf = pd.read_excel(\"Tarikh.xlsx\", header=None)\ndf.dropna(inplace=True)\ndf[0] = df[0].apply(lambda x: str(x).replace(\"/\", \"-\"))\n\n\ndf.iloc[0][0]\n\n'1401/01/01'\n\n\n\n\"1401/01/01\".replace(\"/\", \"\")\n# import re\n# re.sub(\"\\\\\", \" \",\"1401/01/01\")\n\n'14010101'\n\n\n\ndf\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n1401-01-01\n\n\n1\n1401-01-02\n\n\n2\n1401-01-03\n\n\n3\n1401-01-04\n\n\n4\n1401-01-05\n\n\n...\n...\n\n\n4573\n1402-07-08\n\n\n4574\n1402-07-08\n\n\n4575\n1402-07-08\n\n\n4576\n1402-07-09\n\n\n4577\n1402-07-08\n\n\n\n\n4412 rows × 1 columns\n\n\n\n\nimport jalali\n\n\njalali.Persian(\"1401-04-24\").gregorian_string()\n\n'2022-7-15'\n\n\n\ndef to_georgian(row):\n    try:\n        return jalali.Persian(str(row)).gregorian_string()\n    except:\n        return \"BAD FORMAT\"\n\n\ndf[\"georgian\"] = df[0].apply(lambda x: to_georgian(x))\n\n\ndf = pd.read_csv(\"events-export-2920092-1697700606932.csv\")\n\n\ndf[\"Time\"].iloc[0]\n\n1693584039.558\n\n\n\nimport json\n\n\nwith open(\"events-export-2920092-1697700859686.json\") as f:\n    j = json.load(f)\n\n\nj[0]\n\n{'event': 'Create Purchase',\n 'properties': {'time': 1693584039.558,\n  'distinct_id': 'mehdiahmadvand123s@gmail.com',\n  '$insert_id': 'd1e5322e31ef4eb69e5d34080537e7f1',\n  '$lib_version': '4.10.0',\n  '$mp_api_endpoint': 'api.mixpanel.com',\n  '$mp_api_timestamp_ms': 1693584039627,\n  'Discount': None,\n  'Plan': 3112720570236792,\n  'Price': 100000,\n  'Status': 'paied',\n  'Words after purchase': 5000,\n  'Words before purchase': 5000,\n  'mp_lib': 'python',\n  'mp_processing_time_ms': 1693584039662},\n 'isExpanded': True}\n\n\n\nimport json\nimport glob\n\n\ndirs = []\nfor i in glob.iglob(\"*.json\"):\n    dirs.append(i)\n\n\nwith open(dirs[3]) as file:\n    j = json.load(file)\n\n\nimport pandas as pd\nimport re\n\n\nwith open(\"discount_codes.txt\") as file:\n    codes = file.readlines()\n\n\ncodes = [x.replace(\"\\n\", \"\") for x in codes]\n\n\ndf = pd.DataFrame({\"codes\": codes})\n\n\ndf.to_excel(\"discount_codes.xlsx\", index=False)\n\n\na = \"\"\"\nhtml\nasdad\nhtml\n\"\"\"\na.strip()\n\n'html\\nasdad\\nhtml'\n\n\n\nif a.strip().startswith(\"html\"):\n    print(a[4:])\n\nl\nasdad\nhtml"
  },
  {
    "objectID": "posts/2021-07-17-finetunin-persian-language-model.html",
    "href": "posts/2021-07-17-finetunin-persian-language-model.html",
    "title": "Persian ULMFIT",
    "section": "",
    "text": "Following the previous post:pretraining Persian AWD LSTM language model. Here we finetune our language model that was trained on Persian corpus for sentiment analysis. I use the ULMFIT approach introduced by Jeremy Howard, Sebastian Ruder and implemented it nicely in the fastai library. for more information about ULMFIT ,and the ideas behind it, you can refer o this post: Introducing state of the art text classification with universal language models\nFirst, we download the model, tokenizer, and the data, which we will use to finetune our language model. after this running this cell, a file named ULMFIT_FA.zip will be downloaded, which includes the model, tokenizer, and the data.\n\n%%capture\nimport gdown\nurl =  \"https://drive.google.com/uc?id=1-VftZs-XxQD6KvmeNT8Io03MU1mQKylO\"\noutput = 'ULMFIT_FA.zip'\ngdown.download(url, output, quiet=False)\n!unzip ULMFIT_FA.zip\n!pip install -U fastai\n!pip install sentencepiece\n\n\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.text.all import *\nimport pandas as pd\nimport pickle\n\nimport fastai\nimport torch\nprint(f\"fastai version: {fastai.__version__}\")\nprint(f\"GPU which is used : {torch.cuda.get_device_name(0)}\")\n\n## parameters for dataloader and tokenizer\nlang = \"fa\"\nbackwards=False\nbs=128\nvocab_sz = 30000\ndrop_mult = 0.5\nnum_workers=18\n## setting up the pathes\nbase = Path(\".\").absolute()\nprint(f\"our base directory: {base}\")\nulmfit_dir = base / \"ULMFIT_FA\"\nprint(f\"our model and data directory: {ulmfit_dir}\")\nlm_fns = [ulmfit_dir / f\"model_out/{lang}_ULMFIT\", ulmfit_dir / f\"model_out/{lang}_ULMFIT_vocab\"]\n\nfastai version: 2.4.1\nGPU which is used : Tesla P100-PCIE-16GB\nour base directory: /content\nour model and data directory: /content/ULMFIT_FA\n\n\nThis is a preview of the data. I choose Snappfood comment data and going to use it for sentiment analysis of the comments.\n\ndf = pd.read_csv(ulmfit_dir / \"snapp.csv\")\nprint(f\"shape of the data: {df.shape}\")\ndf.sample(5)\n\nshape of the data: (60211, 2)\n\n\n\n\n\n\n\n\n\ncomment\nlabel_id\n\n\n\n\n51993\nغذا سرد شده و بهم ریخته شده بود برنج‌ها خمیر و قابل خوردن نبود . مخلفات همگی بهم ریخته و نامنظم شده بود و اصلا قیافه خوبی ن داشت سبزی ریحون پخته شده بود .\n1\n\n\n4507\nمتاسفانه بجای شمع 3 عدد 2 ارسال شده بود\n0\n\n\n25114\nواقعا کارتون بیسته دقیق و سریع خدا برکت بده\n0\n\n\n25560\nدر حدی سرد شده بود ک قابل خوردن نبود\n1\n\n\n59813\nکتف مرغ رو اینقدر پودر سوخاری زدن که شده اندازه ران . اونوقت یه تکه هم حسابش کردن . همه چیز گرون شده شما هم گرون کن نه اینکه از سر و ته کار بزنی .\n1\n\n\n\n\n\n\n\nleading pretrained SentencePiece for tokenization. Then create a data loader for feeding the language model learner.\n\ntok = SentencePieceTokenizer(lang=\"fa\", max_vocab_sz=vocab_sz, sp_model=ulmfit_dir / \"spm/spm.model\")\n\n\ndblock_lm = DataBlock(\n    blocks=(TextBlock.from_df('comment', is_lm=True, tok=tok,backwards=False)),\n    get_x=ColReader('text'))\n\n\ndls_lm = dblock_lm.dataloaders(df, bs=bs)\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  return array(a, dtype, copy=False, order=order)\n\n\n\ndls_lm.show_batch(max_n=4)\n\n\n\n\n\ntext\ntext_\n\n\n\n\n0\n▁xxbos ▁خیلی ▁تاخیر ▁داشت ▁رسوند ن ▁غذا ▁غذا ▁ها ▁کاملا ▁سرد ▁شده ▁بود ▁و ▁نون ▁ها ▁خمیر ▁xxbos ▁وقتی ▁کیک ▁رسید ▁دستم ▁له ▁له ▁شده ▁شده ▁بود ▁و ▁کاملا ▁از ▁بین ▁رفته ▁بود ▁xxbos ▁خمیر ▁پای ▁سیب ▁ مونده ▁بود ▁. ▁شیرینی ش ▁زیاد ▁بود ▁. ▁مزه ▁شو ▁دوست ▁نداشتم ▁xxbos ▁پیک ▁بسیار ▁مودب ▁و ▁سریع ▁بودن ▁تا ▁پشت ▁در ▁واحد ▁اومدن ▁بالا ▁و ▁تحویل ▁دادن ▁. ▁xxbos ▁بد ▁بود ▁غذاها ▁افتضاح\n▁خیلی ▁تاخیر ▁داشت ▁رسوند ن ▁غذا ▁غذا ▁ها ▁کاملا ▁سرد ▁شده ▁بود ▁و ▁نون ▁ها ▁خمیر ▁xxbos ▁وقتی ▁کیک ▁رسید ▁دستم ▁له ▁له ▁شده ▁شده ▁بود ▁و ▁کاملا ▁از ▁بین ▁رفته ▁بود ▁xxbos ▁خمیر ▁پای ▁سیب ▁ مونده ▁بود ▁. ▁شیرینی ش ▁زیاد ▁بود ▁. ▁مزه ▁شو ▁دوست ▁نداشتم ▁xxbos ▁پیک ▁بسیار ▁مودب ▁و ▁سریع ▁بودن ▁تا ▁پشت ▁در ▁واحد ▁اومدن ▁بالا ▁و ▁تحویل ▁دادن ▁. ▁xxbos ▁بد ▁بود ▁غذاها ▁افتضاح ▁تر\n\n\n1\n▁بود ▁خیلی ▁بهتر ▁میشد ▁. ▁xxbos ▁سالاد ▁سزار ▁بدون ▁سس ▁سزار ▁! ▁؟ ▁نمی دونم ▁فراموش ▁کرده ▁بودن ▁بفرست ن ▁یا ▁همیشه ▁همین ه ▁. ▁xxbos ▁باس پاس ▁از ▁کیفیت ▁خوب ▁غذاها ▁، ▁لطفا ▁به ▁منظور ▁صرفه ▁جویی ▁در ▁مصرف ▁کاغذ ▁، ▁منوی ▁کاغذی ▁همراه ▁غذا ▁ارسال ▁نفر ما ی ید ▁. ▁xxbos ▁شیر کا کایو ▁ارسالی ▁فردا ▁من قضی ▁میشود ▁و ▁طعم ▁بدی ▁دارد ▁xxbos ▁پیتزا ▁بیکن ▁فوق ▁الع اد س\n▁خیلی ▁بهتر ▁میشد ▁. ▁xxbos ▁سالاد ▁سزار ▁بدون ▁سس ▁سزار ▁! ▁؟ ▁نمی دونم ▁فراموش ▁کرده ▁بودن ▁بفرست ن ▁یا ▁همیشه ▁همین ه ▁. ▁xxbos ▁باس پاس ▁از ▁کیفیت ▁خوب ▁غذاها ▁، ▁لطفا ▁به ▁منظور ▁صرفه ▁جویی ▁در ▁مصرف ▁کاغذ ▁، ▁منوی ▁کاغذی ▁همراه ▁غذا ▁ارسال ▁نفر ما ی ید ▁. ▁xxbos ▁شیر کا کایو ▁ارسالی ▁فردا ▁من قضی ▁میشود ▁و ▁طعم ▁بدی ▁دارد ▁xxbos ▁پیتزا ▁بیکن ▁فوق ▁الع اد س ▁ولی\n\n\n2\n▁. ▁فقط ▁خلال ▁بادام ▁را ▁فراموش ▁کردند ▁xxbos ▁من ▁چندین ▁بار ▁از ▁این ▁نانوایی ▁خرید ▁کردم ▁و ▁همیشه ▁عالی ▁بوده ▁ولی ▁متاسفانه ▁امروز ▁بسی از ▁خمیر ▁بود ▁و ▁اصلا ▁قابل ▁استفاده ▁نبود ▁xxbos ▁متوسط ▁خوب ▁بود ▁، ▁ما ▁دو ▁نفری ▁غذا ▁رو ▁خورد یم ▁تازه ▁زیاد ▁هم ▁اومد ▁، ▁مرسی ▁xxbos ▁فاصله ▁ما ▁خیلی ▁کم ه ▁ولی ▁پول ▁پیک ▁؟ ▁مالیات ▁؟ ▁xxbos ▁متاسف م ▁براتون ▁توی ▁توضیحات ▁نوشت ید ▁جوجه ▁کباب\n▁فقط ▁خلال ▁بادام ▁را ▁فراموش ▁کردند ▁xxbos ▁من ▁چندین ▁بار ▁از ▁این ▁نانوایی ▁خرید ▁کردم ▁و ▁همیشه ▁عالی ▁بوده ▁ولی ▁متاسفانه ▁امروز ▁بسی از ▁خمیر ▁بود ▁و ▁اصلا ▁قابل ▁استفاده ▁نبود ▁xxbos ▁متوسط ▁خوب ▁بود ▁، ▁ما ▁دو ▁نفری ▁غذا ▁رو ▁خورد یم ▁تازه ▁زیاد ▁هم ▁اومد ▁، ▁مرسی ▁xxbos ▁فاصله ▁ما ▁خیلی ▁کم ه ▁ولی ▁پول ▁پیک ▁؟ ▁مالیات ▁؟ ▁xxbos ▁متاسف م ▁براتون ▁توی ▁توضیحات ▁نوشت ید ▁جوجه ▁کباب ▁800\n\n\n3\nه ▁فقط ▁یه ▁مقدار ▁خیلی ▁خیلی ▁کم ▁روش ▁گردو ▁خورد ▁شده ▁ریخته ▁خیلی ▁بیخود ▁بود ▁نسبت ▁به ▁پولی ▁که ▁گرفتن ▁همین ▁سالاد ▁و ▁به ▁اسم ▁سالاد ▁فصل ▁همه ▁جا ▁داره ▁20 ▁تومن ▁میده ▁xxbos ▁عالی ▁بود ▁واقعا ▁. ▁من ▁جایی ▁دیگه ▁پیتزا ▁چان و ▁خوردم ▁ولی ▁پیتزا ی ▁شما ▁باعث ▁شد ▁بفهمم ▁قبلی ا ▁پیتزا ▁نبودن ▁که ▁خوردم ▁. ▁واقعا ▁عالی ▁بود ▁ممنون ▁از ▁شما ▁xxbos ▁آقا ▁ما ▁ساعت ▁10 ▁شب ▁گ\n▁فقط ▁یه ▁مقدار ▁خیلی ▁خیلی ▁کم ▁روش ▁گردو ▁خورد ▁شده ▁ریخته ▁خیلی ▁بیخود ▁بود ▁نسبت ▁به ▁پولی ▁که ▁گرفتن ▁همین ▁سالاد ▁و ▁به ▁اسم ▁سالاد ▁فصل ▁همه ▁جا ▁داره ▁20 ▁تومن ▁میده ▁xxbos ▁عالی ▁بود ▁واقعا ▁. ▁من ▁جایی ▁دیگه ▁پیتزا ▁چان و ▁خوردم ▁ولی ▁پیتزا ی ▁شما ▁باعث ▁شد ▁بفهمم ▁قبلی ا ▁پیتزا ▁نبودن ▁که ▁خوردم ▁. ▁واقعا ▁عالی ▁بود ▁ممنون ▁از ▁شما ▁xxbos ▁آقا ▁ما ▁ساعت ▁10 ▁شب ▁گ شن\n\n\n\n\n\n\nlearn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=drop_mult, pretrained=True, pretrained_fnames=lm_fns, \n                               metrics=[accuracy, Perplexity()]).to_fp16()\n\nUsing learning rate finder of fastai. Here we plot the loss versus the learning rates. We’re interested in finding a good order of magnitude of the learning rate, so we plot with a log scale. Then, we choose a value that is approximately in the middle of the sharpest downward slope.\nFor more information on the finding the good learning rate you can refer to this post: how do you find a good learning rate\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(valley=0.0005754399462603033)\n\n\n\n\n\nNext, we finetune the model. By default, a pretrained Learner is in a frozen state, meaning that only the head of the model will train while the body stays frozen.\n\nlr = 1e-3\nlr *= bs/48\nlearn.fit_one_cycle(1, lr, moms=(0.8,0.7,0.8))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n4.431968\n4.142324\n0.249683\n62.948944\n01:39\n\n\n\n\n\nWe can them fine-tune the model after unfreezing\n\nlearn.unfreeze()\nlearn.fit_one_cycle(6, lr, moms=(0.8,0.7,0.8))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nperplexity\ntime\n\n\n\n\n0\n4.061007\n3.951483\n0.270287\n52.012421\n01:46\n\n\n1\n3.865995\n3.788972\n0.287438\n44.210945\n01:47\n\n\n2\n3.646721\n3.685658\n0.299375\n39.871330\n01:47\n\n\n3\n3.451701\n3.635723\n0.305642\n37.929253\n01:47\n\n\n4\n3.253018\n3.620441\n0.308691\n37.354046\n01:46\n\n\n5\n3.116226\n3.625103\n0.309143\n37.528580\n01:46\n\n\n\n\n\nOnce this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the encoder. We can save it with save_encoder\n\nlearn.save_encoder(ulmfit_dir / 'finetuned')\n\nHere we gather our data for text classification almost exactly like before:\n\ndblocks_clas = DataBlock(blocks=(TextBlock.from_df('comment', tok=tok, vocab=dls_lm.vocab, backwards=backwards), CategoryBlock),\n                    get_x=ColReader('text'),\n                    get_y=ColReader('label_id'),\n                    )\ndls_clas = dblocks_clas.dataloaders(df, bs=bs, num_workers=num_workers)\n\n\n\n\n/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  return array(a, dtype, copy=False, order=order)\n\n\n\ndls_clas.show_batch(max_n=4)\n\n\n\n\n\ntext\ncategory\n\n\n\n\n0\n▁xxbos ▁اسنپ ▁مثل ▁همیشه ▁عالی ▁این ▁همه ▁راه ▁رو ▁سریع ▁اومد ▁داغ ▁اورد ▁ولی ▁واقعا ▁از ▁رستوران ▁ایتالیایی ▁مثل ▁شما ▁بعید ه ▁ع ذا تون ▁طعم ▁خوبی ▁نده . ▁قیمت ▁رو ▁شده ▁بالا ▁ببر ین ▁ولی ▁غذا ▁رو ▁خوش ▁طعم ▁درست ▁کنید ▁و ▁باور ▁کنید ▁اگه ▁غذا ▁رو ▁با ▁کیفیت ▁خوب ▁درست ▁کنید ▁باز ▁هم ▁چندین ▁درصد ▁سود ▁میکنید ▁به ▁علاوه ▁این ▁که ▁مشتری ▁بیشتر ▁و ▁بیشتر ▁جذب ▁میکنید ▁اشک لات ▁غذا ▁ ۱ ) ▁خمیر ▁بد ▁طعم ▁و ▁کلفت ▁ ۲ ) ▁پنیر ش ▁واقعا ▁طعم ▁خوبی ▁نمیداد ▁طعم ▁کره ▁میداد ▁ ۳ ) ▁سس ▁بار بیک رو ▁هم ▁نداشت ▁فقط ▁مرغ ▁ها ▁یه ▁مقدار ▁طعم ▁باربیکیو ▁می دادن ▁ ۴ ) ▁چی ▁بگم ▁دیگه ▁حتی ▁روی ▁خمیر ▁هم ▁زیر ▁مواد ▁سس ▁نداشت ▁من ▁از ▁یک ▁دوست xxunk ▁که ▁آشپز ه ▁حرفه ▁ای ▁هست ▁این ▁دستور ▁رو ▁گرفتم ▁تو ▁خونه ▁هم ▁درست ▁کردم ▁طعم ش ▁معر که ▁میشه ▁و\n1\n\n\n1\n▁xxbos ▁سلام ▁مثل ▁تمام ▁سفارش ▁های ▁قبلی ▁عالی ▁و ▁بی ▁نظیر ▁بود ▁کدو ▁و ▁بادمجان ▁پیتزا ی ▁سبزیجات ▁به ▁خوبی ▁گریل ▁شده ▁بود ▁و ▁طعم ▁خوبی ▁به ▁پیتزا ▁داده ▁بود ▁و ▁بر ▁خلاف ▁پیتزا ▁های ▁سبزیجات ی ▁که ▁قبلا ▁خورده ▁بودم ▁( xxunk ▁با ▁کدو ▁و ▁بادمجان ) ▁این ▁یکی ▁خیلی ▁طعم ▁بهتری ▁داشت ▁و ▁نظر مو ▁نسبت ▁به ▁استفاده ▁ی ▁کدو ▁و ▁بادمجان ▁در ▁پیتزا ▁عوض ▁کرد ▁و ▁خوشم ▁اومد ▁از ش : ) ؛ ) ▁چی کن ▁چیز ▁هم ▁فوق ▁العاده ▁عالی ▁بود ▁اولین ▁باری ▁بود ▁که ▁امتحان ش ▁میکردم ▁اما ▁از ▁طعم ش ▁خیلی ▁خوشم ▁اومد ▁و ▁به ▁نظرم ▁طعم ▁ ادویه ▁هایی ▁که ▁در ▁تهیه ▁مرغ ش ▁یه ▁کار ▁رفته ▁بود ▁در ▁بقیه ▁مواد ▁و ▁پنیر ش ▁هم ▁پخش ▁شده ▁بود ▁و ▁بی ▁نظیر ▁شده ▁بود ▁یه ▁طعم ▁متفاوت ▁و ▁دوست ▁داشتنی ▁بود ▁که ▁آدم ▁دوست ▁داره ▁همیشه ▁امتحان ش ▁کنه : ) ؛ ) ▁آلپ\n0\n\n\n2\n▁xxbos ▁پیشنهاد ▁برای ▁زود ▁فود ▁= ▁در xxunk ▁هایی ▁که ▁برای ▁میزان ▁رضایت ▁از ▁مشتری ▁بکار ▁برد ید ▁یا ▁راضی ▁وجود ▁دارد ▁یا ▁ناراضی ! ▁یعنی ▁یا ▁ ۰ ▁یا ▁ ۱۰۰ ▁xxrep ▁3 ▁! ▁خب ▁اگر ▁کسی ▁بین xxunk ▁رضایت ▁داشت ▁کدوم ▁رو ▁انتخاب ▁کنه ؟ ▁بنظرم ▁متداول ▁بیشتر ▁نظر ▁سنجی ▁ها ▁خوب ▁/ ▁متوسط ▁/ ▁بد ▁میتونه ▁باشه . ▁رضایت ▁من ▁هم ▁از ▁دیزی ▁بهار ▁= ▁متوسط ▁پیاز ▁در ▁بسته ▁بندی ▁نبود ▁که ▁عموما ▁دیزی ▁ها ▁می گذارند . ▁زرد ▁چوب ه ▁اش ▁زیاد ▁بود ▁و ▁خیلی ▁رو ▁طمع ▁تاثیر ▁گذاشته ▁بود . ▁میزان ▁کمی ▁ گوجه ▁و ▁سیب ▁زمینی ▁و ▁نخود ▁لوبیا ش ▁مناسب ▁بود ▁اما ▁هنوز ▁از ▁نظر ▁کیفی ▁کار ▁داشت ▁و ▁دیزی ▁بیشتر ▁جا ▁می افتاد ▁لذیذ ▁تر ▁میشد . ▁نون ▁سنگ ک ▁هم ▁سوختگی ▁اش ▁زیاد ▁بود . ▁خودمون ▁رفتیم ▁از ▁سر کو چه ▁سنگ ک ▁خرید یم ▁تا ▁لذت ▁دیزی ▁خوردن ▁با ▁سنگ\n0\n\n\n3\n▁xxbos ▁سلام ▁متاسفانه ▁با ▁توجه ▁به ▁اینکه ▁اصول ا ▁خودم ▁یا xxunk ▁م ▁برای ▁تهیه ▁نون ▁بصورت ▁حضور ▁ی ▁مراجعه ▁می ▁کنیم ▁و ▁با ▁توجه ▁به ▁اینکه ▁اولین ▁بار ▁نیست ▁که ▁نون ▁دو ▁رو ▁کنجد ▁می ▁خرم ▁مقدار ▁کنجد ها ش ▁نسبت ▁به ▁قیمت ▁خیلی ▁کم ▁بود ▁و ▁این ▁همه ▁پول ▁جهت ▁ارسال ▁نون ▁رو ▁اصول ا ▁کمی ▁که ▁از ▁تنور ▁در ▁میارن ▁باد ▁می ▁خوره ▁بعد ▁داخل ▁نایلون ▁می ▁گذارند ▁ولی ▁متاسفانه ▁بدون ▁دلسوز ی ▁جمع ▁آوری ▁نون ▁انجام ▁شده ▁بود ▁وقتی ▁به ▁دستم ▁رسید ▁پلاستیک ▁و ▁نون ▁کاملا ▁خیس ▁بود ▁و ▁حالم ▁رو ▁بد ▁کرد ▁سر جمع ▁راضی ▁نبودم ▁چون ▁هم ▁پول ▁بدی ▁هم ▁ببینی ▁از ▁لحاظ ▁مقدار ▁کنجد ▁بکار ▁برده ▁شده ▁کم ▁بود ▁و ▁هم ▁نون ▁خیس ▁تحویل ▁بگیری ▁خوشم ▁نیومد ▁اولین ▁تجربه ▁= ▁تلخ ▁ضمنا ▁پیشنهاد م ▁به ▁نانوایی ▁ها ▁اینکه ▁چندین ▁کیسه ▁پارچه ▁ای ▁تهیه ▁کنند ▁و ▁برای ▁تحویل ▁نان ▁به ▁مشتری ▁از ▁این ▁گونه\n1\n\n\n\n\n\nThe main difference is that we have to use the exact same vocabulary as when we were fine-tuning our language model, or the weights learned won’t make any sense. We pass that vocabulary with vocab.\nThen we can define our text classifier like before:\nDefing metrics: we use accuracy and F1Score\n\nmetrics=[accuracy,F1Score()]\nlearn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=1, pretrained=False, \n                               metrics=metrics).to_fp16()\n\nlearn = learn.load_encoder(ulmfit_dir / 'finetuned')\nlearn.freeze()\n\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(valley=tensor(0.0012))\n\n\n\n\n\n\nlr = 1e-3\nlearn.fit_one_cycle(1, lr, moms=(0.8,0.7,0.8))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nf1_score\ntime\n\n\n\n\n0\n0.411142\n0.329208\n0.864807\n0.876198\n01:39\n\n\n\n\n\nThe last step is to train with discriminative learning rates and gradual unfreezing. In computer vision, we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference.\n\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(lr/(2.6**4),lr), moms=(0.8,0.7,0.8))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nf1_score\ntime\n\n\n\n\n0\n0.378707\n0.317943\n0.867796\n0.879321\n01:41\n\n\n\n\n\n\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(lr/2/(2.6**4),lr/2), moms=(0.8,0.7,0.8))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nf1_score\ntime\n\n\n\n\n0\n0.375836\n0.310461\n0.871533\n0.881411\n01:37\n\n\n\n\n\n\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(lr/10/(2.6**4),lr/10), moms=(0.8,0.7,0.8))\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\nf1_score\ntime\n\n\n\n\n0\n0.374451\n0.309468\n0.872197\n0.883294\n01:39\n\n\n1\n0.360467\n0.308152\n0.872862\n0.882547\n01:38\n\n\n\n\n\nWith this approach we got better results than parsbert on this datset. note that we didn’t do so much of preprocesing compare to parsbert finetunning.\nYou can check out ULMFIT in other languages in this repo: fastai_ulmfit, which is an excellent work of Florian.\nI really enjoyed working with fastai, and especially this approach, and I hope that others contribute to this project to make it even better.\nHere are other useful links :\n\nTransfer learning in text\nTransformers with fastai\nfasthugs"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Let’s forget …\nHey folks, My name is Saied Alimoradi. I’m a professional Machine Learning Engineer with over 4 years of experience in AI and Data Science. Currently, I work at DataCoLab Company, where I employ my skills to solve complex problems and develop innovative solutions.\nI am the founder of Khodnevis, which is the first Persian AI copywriter. This project signifies my passion for blending AI technology with language and communication, creating a unique platform that supports Persian content creation.\nIn addition to my work in AI and Data Science, I have also worked in MLOps, focusing on the deployment of AI model’s lifecycle. This experience has provided me with a comprehensive understanding of AI implementation, enabling me to streamline processes and enhance efficiency in my projects.\nI am always open to collaboration and new opportunities. Feel free to contact me at saied.alimoradi@gmail.com for any inquiries or potential projects. I look forward to connecting with you!"
  },
  {
    "objectID": "posts/test-notebook.html",
    "href": "posts/test-notebook.html",
    "title": "test-notebook",
    "section": "",
    "text": "df = pd.read_json(\"users_export_phone.json\")\n\n\ndf.drop_duplicates(subset=[\"phone_number\"], inplace=True)\ndf.dropna(subset=[\"phone_number\"], inplace=True)\n\n\ntxt_num = \"\\n\".join(df[\"phone_number\"].astype(\"str\").tolist())\nwith open(\"phone_numbers.txt\", \"w\") as file:\n    file .write(txt_num)\n\n\ndf_all = pd.DataFrame()\nfor i in glob.iglob(\"word_data/*.json\"):\n    if \"article\" in i:\n        df = pd.read_json(i)\n        if \"count\" in df.columns.tolist():\n            df.rename(columns={\"count\":\"words\"}, inplace=True)\n        df_all = pd.concat([df, df_all], ignore_index=True)\n    else:\n        pass\n\n\ndf_all[\"created\"] = pd.to_datetime(df_all[\"created\"])\n\n\ndf_all = df_all[df_all[\"created\"] &gt; \"2023-07-23\"]\n\n\ndf_all[\"month\"] = df_all[\"created\"].dt.month\ndf_all.groupby(\"month\").agg({\"complete\": \"sum\", \"words\": \"sum\"})\n\n\n\n\n\n\n\n\ncomplete\nwords\n\n\nmonth\n\n\n\n\n\n\n1\n1972\n1944453\n\n\n7\n508\n663098\n\n\n8\n1310\n1688996\n\n\n9\n1479\n1944561\n\n\n10\n1804\n2373311\n\n\n11\n1935\n1904306\n\n\n12\n2876\n2759498\n\n\n\n\n\n\n\n\ndd = df_all[df_all[\"created\"] &gt; \"1-10-2023\"]\ndd[\"words\"].max()\n\n8839\n\n\n\ndf_month = df_all.groupby(\"week\").agg({\"words\": \"sum\"}).reset_index()\ndf_day = df_all.groupby(\"day\").agg({\"words\": \"sum\"}).reset_index()\n\n\na= dd.groupby(pd.Grouper(key='created', freq='D')).agg({\"words\": \"mean\"})\nprint(a[\"words\"].mean())\n\n614.6506297601618\n\n\n\na= df_all.groupby(pd.Grouper(key='created', freq='W')).agg({\"words\": \"mean\"})\nprint(a[\"words\"].mean())\n\n591.223619913266\n\n\n\ndef weekinmonth(dates):\n    \"\"\"Get week number in a month.\n    \n    Parameters: \n        dates (pd.Series): Series of dates.\n    Returns: \n        pd.Series: Week number in a month.\n    \"\"\"\n    firstday_in_month = dates - pd.to_timedelta(dates.dt.day - 1, unit='d')\n    return (dates.dt.day-1 + firstday_in_month.dt.weekday) // 7 + 1\nweekinmonth(df[\"created\"])\n\nAttributeError: Can only use .dt accessor with datetimelike values\n\n\n\ndf = pd.read_excel(\"Tarikh.xlsx\", header=None)\ndf.dropna(inplace=True)\ndf[0] = df[0].apply(lambda x: str(x).replace(\"/\", \"-\"))\n\n\ndf.iloc[0][0]\n\n'1401/01/01'\n\n\n\n\"1401/01/01\".replace(\"/\", \"\")\n# import re\n# re.sub(\"\\\\\", \" \",\"1401/01/01\")\n\n'14010101'\n\n\n\ndf\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n1401-01-01\n\n\n1\n1401-01-02\n\n\n2\n1401-01-03\n\n\n3\n1401-01-04\n\n\n4\n1401-01-05\n\n\n...\n...\n\n\n4573\n1402-07-08\n\n\n4574\n1402-07-08\n\n\n4575\n1402-07-08\n\n\n4576\n1402-07-09\n\n\n4577\n1402-07-08\n\n\n\n\n4412 rows × 1 columns\n\n\n\n\nimport jalali\n\n\njalali.Persian(\"1401-04-24\").gregorian_string()\n\n'2022-7-15'\n\n\n\ndef to_georgian(row):\n    try:\n        return jalali.Persian(str(row)).gregorian_string()\n    except:\n        return \"BAD FORMAT\"\n\n\ndf[\"georgian\"] = df[0].apply(lambda x: to_georgian(x))\n\n\ndf = pd.read_csv(\"events-export-2920092-1697700606932.csv\")\n\n\ndf[\"Time\"].iloc[0]\n\n1693584039.558\n\n\n\nimport json\n\n\nwith open(\"events-export-2920092-1697700859686.json\") as f:\n    j = json.load(f)\n\n\nj[0]\n\n{'event': 'Create Purchase',\n 'properties': {'time': 1693584039.558,\n  'distinct_id': 'mehdiahmadvand123s@gmail.com',\n  '$insert_id': 'd1e5322e31ef4eb69e5d34080537e7f1',\n  '$lib_version': '4.10.0',\n  '$mp_api_endpoint': 'api.mixpanel.com',\n  '$mp_api_timestamp_ms': 1693584039627,\n  'Discount': None,\n  'Plan': 3112720570236792,\n  'Price': 100000,\n  'Status': 'paied',\n  'Words after purchase': 5000,\n  'Words before purchase': 5000,\n  'mp_lib': 'python',\n  'mp_processing_time_ms': 1693584039662},\n 'isExpanded': True}\n\n\n\nimport json\nimport glob\n\n\ndirs = []\nfor i in glob.iglob(\"*.json\"):\n    dirs.append(i)\n\n\nwith open(dirs[3]) as file:\n    j = json.load(file)\n\n\nimport pandas as pd\nimport re\n\n\nwith open(\"discount_codes.txt\") as file:\n    codes = file.readlines()\n\n\ncodes = [x.replace(\"\\n\", \"\") for x in codes]\n\n\ndf = pd.DataFrame({\"codes\": codes})\n\n\ndf.to_excel(\"discount_codes.xlsx\", index=False)\n\n\na = \"\"\"\nhtml\nasdad\nhtml\n\"\"\"\na.strip()\n\n'html\\nasdad\\nhtml'\n\n\n\nif a.strip().startswith(\"html\"):\n    print(a[4:])\n\nl\nasdad\nhtml"
  }
]